/-
 - Created in 2025 by Rémy Degenne
-/

import VersoManual

open Verso.Genre Manual Verso.Genre.Manual.InlineLean Verso.Code.External

set_option pp.rawOnError true

set_option verso.exampleProject "src/"

set_option verso.exampleModule "Martingales.Basic"

#doc (Manual) "Various probability definitions" =>
%%%
htmlSplit := .never
%%%


This section contains pointers to the Mathlib definitions for several probability objects.
That list might be out of date when you read this! Look around in the [documentation](https://leanprover-community.github.io/mathlib4_docs/).

# CDF, pdf, variance, moments

Here is a list of common concepts about real probability distributions.

- Probability density function of a random variable `X : Ω → E` in a space with measure `P : Measure Ω`, with respect to measure `Q : Measure E`: `pdf X P Q`.
- Cumulative distribution function of `P : Measure ℝ`: `cdf P`. This is a `StieltjesFunction`, a monotone right continuous real function.
- Expectation of `X` under `P`: `P[X]`.
- Variance: `variance X P`.
- Moment of order `p`: `moment X p P`.
- Central moment of order `p`: `centralMoment X p P`.
- Moment generating function of `X` under `P` at `t : ℝ`: `mgf X P t`.
- Cumulant generating function: `cgf X P t`.

# Known probability distributions

The Probability/Distributions folder of Mathlib contains several common probability laws: Exponential, Gamma, Gaussian (only in `ℝ`), Geometric, Pareto, Poisson, Uniform.
Other measures that can be used to build probability distributions are defined elsewhere, in the MeasureTheory folder: counting measure, Dirac, Lebesgue measure.

For example, the Gaussian distribution on the real line with mean `μ` and variance `v` is a {anchorTerm Gaussian}`Measure ℝ`:
```anchor Gaussian
example (μ : ℝ) (v : ℝ≥0) : Measure ℝ := gaussianReal μ v
```
The definition is followed by an instance `IsProbabilityMeasure (gaussianReal μ v)` that states that this is a probability measure. It is defined as the Dirac distribution for `v = 0`.
The file where the Gaussian is defined also contains properties of its probability density function.

As another example, to take the uniform distribution on a finite set `s : Set Ω`, use `uniformOn s : Measure Ω`.

# Identically distributed

`IdentDistrib X Y P Q` (or `IdentDistrib X Y` in `MeasureSpace`) states that `X` and `Y` are identically distributed.

# Conditioning

## Conditional probability

The probability of a set `s` conditioned on another set `t` for the measure `P` is `P[s|t]`, which equals `P (s ∩ t) / P t`.
Conditioning on `t` defines a new measure named `cond P t`, denoted by `P[|t]`. With that notation, `P[s|t] = P[|t] s`.

For `X : Ω → E` and `x : E`, we write `P[|X ← x]` for the probability measure conditioned on `X = x`. It is notation for `P[|X ⁻¹' {x}]`.
This is meaningful only if `X = x` has non-zero probability, so that definition is mostly useful for discrete probability.

## Conditional expectation

The conditional expectation of a random variable `Y : Ω → F` given a sigma-algebra `m` of `Ω` is `condexp m P Y`, with notation `P[Y | m]`. This is a random variable `Ω → F`, and it is `m`-measurable.
The conditional expectation of `Y` given `X : Ω → E` can be obtained with `P[Y | mE.comap X]`, in which `mE : MeasurableSpace E` is the sigma-algebra on `E`.
The sigma-algebra `mE.comap X` (or `MeasurableSpace.comap X mE`) on `Ω` is the sigma-algebra generated by `X`. It is the smallest sigma-algebra for which all the sets `X ⁻¹' s` are measurable, for `s` ranging over the measurable sets on `E`.

We have special notation for the conditional expectation of the real indicator of a set `s : Set Ω`: `P⟦s | m⟧ : Ω → ℝ`.

## Conditional probability distribution

The regular conditional probability distribution of `Y : Ω → F` given `X : Ω → E`, for `F` standard Borel, is denoted by `condDistrib Y X P` (for a measure `P` on `Ω`).
This is a Markov kernel from `E` to `F` (see further down) such that for all measurable sets `s` of `F`, for `P`-almost every `ω : Ω`, `condDistrib Y X P (X ω) s` is equal to `P⟦Y ⁻¹' s|mE.comap X⟧` (up to a mismatch in types: `ℝ≥0∞` for `condDistrib` versus `ℝ` for the conditional expectation of the indicator).

# Independence

Mathlib has several definitions for independence. We can talk about independence of random variables, of sets, of sigma-algebras.
The definitions also differ depending on whether we consider only two random variables of an indexed family.
Finally, there are also conditional variants of all those definitions.

## Unconditional independence

Two independent random variables can be defined like this:
```anchor Indep
variable {Ω : Type*} [MeasurableSpace Ω] {P : Measure Ω}
  {X : Ω → ℝ} {Y : Ω → ℕ} (hX : Measurable X) (hY : Measurable Y)
  (hXY : IndepFun X Y P)
```
On a measure space, we can write `IndepFun X Y` without the measure argument.

For a family `X : ι → Ω → ℝ` of independent random variables, use `iIndepFun`.

To express independence of sets, use `IndepSet` (two sets) and `iIndepSet` (family of sets). For sigma-algebras, `Indep` and `iIndep`.

## Conditional independence

The way to write that two random variables `X : Ω → E` and `Y : Ω → F` are conditionally independent given a sub-sigma-algebra `m` with respect to the measure `P` is `CondIndepFun m hm X Y P`, in which `hm : m ≤ mΩ` is a proof that `m` is a sub-sigma-algebra of the sigma-algebra `mΩ` of `Ω`.
We would omit the measure argument in a `MeasureSpace`.
That definition requires that `Ω` be standard Borel space.

To write that `X : Ω → E` and `Y : Ω → F` are conditionally independent given a third random variable `Z : Ω → G` (with measurability assumption `hZ : Measurable Z`), write that they are independent given the sigma-algebra generated by `Z`.
That is, for `mG : MeasurableSpace G` the sigma-algebra on `G`, write `CondIndepFun (mG.comap Z) hZ.comap_le X Y P`.
As of writing this blog post, there is no shorter spelling of that fact.

For families of functions, use `iCondIndepFun`.
For sets, use `CondIndepSet` (two sets) and `iCondIndepSet` (family of sets). For sigma-algebras, `CondIndep` and `iCondIndep`.

# Stochastic processes, filtrations, martingales

A stochastic process with real values (for example) is a function `X : ι → Ω → ℝ` from an index set `ι` to random variables `Ω → ℝ`.

A filtration on the index set can be defined with `ℱ : Filtration ι m`, in which `m` is a sigma-algebra on `Ω`. `ℱ` is a monotone sequence of sub-sigma-algebras of `m`. The sigma-algebra at index `i` is `ℱ i`.

We can state that a process `X` is adapted to the filtration `ℱ` with `adapted ℱ X`.
We can write that it is a martingale with respect to `ℱ` and the measure `P : Measure Ω` with `Martingale X ℱ P`.

A stopping time with respect to a filtration `ℱ` is a function `τ : Ω → ι` such that for all `i`, the preimage of `{j | j ≤ i}` along `τ` is measurable with respect to `ℱ i`.
`IsStoppingTime ℱ τ` states that `τ` is a stopping time.

Remark: there is an issue with the current definition of stopping times, which is that if the index set of the filtration `ι` is the set of natural numbers `ℕ` then `τ` has to take values in `ℕ`. In particular it cannot be infinite.
That can cause friction with informal stopping times which are often understood to be infinite when they are not well defined.

# Transition kernels

A transition kernel `κ` from a measurable space `E` to a measurable space `F` is a measurable function from `E` to `Measure F`. That is, for every measurable set `s` of `F`, the function `fun x ↦ κ x s` is measurable.
Transition kernels are represented in Mathlib by the type `Kernel E F`.
A kernel such that all measures in its image are probability measures is called a Markov kernel. This is denoted by `IsMarkovKernel κ`.
There are other typeclasses for kernels that are finite (`IsFiniteKernel`) or s-finite (countable sum of finite, `IsSFiniteKernel`).

Kernels are perhaps more widely used in Mathlib than one would expect. For example independence and conditional independence are both expressed as special cases of a notion of independence with respect to a kernel and a measure.
